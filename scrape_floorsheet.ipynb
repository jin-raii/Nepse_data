{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(driver, date):\n",
    "    \"\"\"\n",
    "    Date in mm/dd/yyyy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://merolagani.com/Floorsheet.aspx\")\n",
    "        try:\n",
    "            alert = driver.switch_to.alert\n",
    "            print(f'Alert detected {alert.text}')\n",
    "            alert.dismis()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        date_element_path = '/html/body/form/div[4]/div[4]/div/div/div[1]/div[4]/input'\n",
    "        search_element_path2 = '/html/body/form/div[4]/div[4]/div/div/div[2]/a[1]'\n",
    "        date_input = driver.find_element(By.XPATH, date_element_path)\n",
    "        search_btn = driver.find_element(By.XPATH, search_element_path2)\n",
    "        # date_input = driver.find_element_by_xpath()\n",
    "        # search_btn = driver.find_element_by_xpath()\n",
    "        date_input.send_keys('02/13/2025')\n",
    "        print(search_btn)\n",
    "        search_btn.click()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # if driver.find_elements(By.XPATH, \"//*[contains(text(), 'Could not find floorsheet matching the search criteria')]\"):\n",
    "    #     print(\"No data found for the given search.\")\n",
    "    #     print(\"Aborting script ......\")\n",
    "    #     sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/15/2025'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_table(driver, table_class):\n",
    "    soup = BeautifulSoup(driver.page_source,'html')\n",
    "    table = soup.find(\"table\", {\"class\":table_class})\n",
    "    tab_data = [[cell.text.replace('\\r', '').replace('\\n', '') for cell in row.find_all([\"th\",\"td\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "    df = pd.DataFrame(tab_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(driver, date):\n",
    "    start_time = datetime.now()\n",
    "    search(driver,'02/13/2025')\n",
    "    df = pd.DataFrame()\n",
    "    while True:\n",
    "        page_table_df = get_page_table(driver, table_class=\"table table-bordered table-striped table-hover sortable\")\n",
    "        print(type(page_table_df))\n",
    "        df = df[page_table_df]\n",
    "        print(df.head())\n",
    "        # df = df.append(page_table_df, ignore_index = True)\n",
    "        try:\n",
    "            next_btn = driver.find_element_by_link_text('Next')\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    print(f\"Time taken to scrape: {datetime.now() - start_time}\")    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    new_df = df.drop_duplicates(keep='first') # Dropping Duplicates\n",
    "    new_header = new_df.iloc[0] # grabing the first row for the header\n",
    "    new_df = new_df[1:] # taking the data lower than the header row\n",
    "    new_df.columns = new_header # setting the header row as the df header\n",
    "    new_df.drop([\"#\"], axis=1, inplace=True)\n",
    "    new_df[\"Rate\"] = new_df[\"Rate\"].apply(lambda x:float(x.replace(\",\", \"\"))) # Convert Rate to Float\n",
    "    new_df[\"Amount\"] = new_df[\"Amount\"].apply(lambda x:float(x.replace(\",\", \"\"))) # Convert Amount to Float\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"e952636e5e679dfa3ef1164e795cce55\", element=\"f.8B3064E7F6F15AD4FEF620C3686D1E7B.d.2956901C5167DD7664389E209191ABE0.e.114\")>\n",
      "Alert Text: Do you want to receive notifications?\n",
      "Message: unexpected alert open: {Alert text : Do you want to receive notifications?}\n",
      "  (Session info: chrome=133.0.6943.98)\n",
      "Stacktrace:\n",
      "#0 0x59ae6ce8fbba <unknown>\n",
      "#1 0x59ae6c92d790 <unknown>\n",
      "#2 0x59ae6c9cb7a0 <unknown>\n",
      "#3 0x59ae6c9a4823 <unknown>\n",
      "#4 0x59ae6c970a88 <unknown>\n",
      "#5 0x59ae6c971bf1 <unknown>\n",
      "#6 0x59ae6ce5915b <unknown>\n",
      "#7 0x59ae6ce5d0e2 <unknown>\n",
      "#8 0x59ae6ce4601c <unknown>\n",
      "#9 0x59ae6ce5dcd4 <unknown>\n",
      "#10 0x59ae6ce2a48f <unknown>\n",
      "#11 0x59ae6ce7e4f8 <unknown>\n",
      "#12 0x59ae6ce7e6c9 <unknown>\n",
      "#13 0x59ae6ce8ea36 <unknown>\n",
      "#14 0x7049e5e9caa4 <unknown>\n",
      "#15 0x7049e5f29c3c <unknown>\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_link_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Get today's date\u001b[39;00m\n\u001b[1;32m      7\u001b[0m search(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02/13/2025\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Search the webpage\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Scraping\u001b[39;00m\n\u001b[1;32m      9\u001b[0m final_df \u001b[38;5;241m=\u001b[39m clean_df(df) \u001b[38;5;66;03m# Cleaning\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[161], line 11\u001b[0m, in \u001b[0;36mscrape_data\u001b[0;34m(driver, date)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# df = df.append(page_table_df, ignore_index = True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     next_btn \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element_by_link_text\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments[0].click();\u001b[39m\u001b[38;5;124m\"\u001b[39m, next_btn)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_link_text'"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument('--disable-notifications')\n",
    "driver = webdriver.Chrome(options=options) # Start Browser\n",
    "\n",
    "date = datetime.today().strftime('%m/%d/%Y') # Get today's date\n",
    "search(driver, '02/13/2025') # Search the webpage\n",
    "df = scrape_data(driver, date) # Scraping\n",
    "final_df = clean_df(df) # Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = date.replace(\"/\", \"_\")\n",
    "final_df.to_csv(f\"data/{file_name}.csv\", index=False) # Save file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
